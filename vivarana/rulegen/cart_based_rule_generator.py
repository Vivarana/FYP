import datetime
from sklearn.metrics import confusion_matrix
import logging
from numpy import *
from pandas.io import json
from rpy2.robjects.packages import importr
import rpy2.robjects as ro
import pandas.rpy.common as com

import rule as rule_classes
import vivarana.constants as constants

logger = logging.getLogger(__name__)

data_types = None
r_code = '''
descendants <- function(nodes, include = TRUE)
{
    n <- length(nodes)
    if (n == 1L) return(matrix(TRUE, 1L, 1L))
    ind <- 1:n
    desc <- matrix(FALSE, n, n)
    if (include) diag(desc) <- TRUE
    parents <- match((nodes %/% 2L), nodes)
    lev <- floor(log(nodes, base = 2))
    desc[1L, 2L:n] <- TRUE
    for (i in max(lev):2L) {
        desc[cbind(ind[parents[lev == i]], ind[lev == i])] <- TRUE
        parents[lev == i] <- parents[parents[lev == i]]
        lev[lev == i] <- i - 1L
    }
    desc
}

mypath <- function(tree, nodes, pretty = 0, print.it = TRUE)
{
    if (!inherits(tree, "rpart"))
        stop("Not a legitimate \\"rpart\\" object")
    splits <- labels(tree, digits=25, pretty = pretty)
    frame <- tree$frame
    n <- row.names(frame)
    node <- as.numeric(n)
    which <- descendants(node)          # ancestors are columns
    path <- list()
    if (missing(nodes)) {
        xy <- rpartco(tree)
        while(length(i <- identify(xy, n = 1L, plot = FALSE)) > 0L) {
            path[[n[i]]] <- path.i <- splits[which[, i]]
            if (print.it) {
                cat("\\n", "node number:", n[i], "\\n")
                cat(paste("  ", path.i), sep = "\\n")
            }
        }
    } else {
        if (length(nodes <- match(nodes, node)) == 0L)
            return(invisible())
        for (i in nodes) {
            path[[n[i]]] <- path.i <- splits[which[, i]]
            if (print.it) {
                cat("\\n", "node number:", n[i], "\\n")
                cat(paste("  ", path.i), sep = "\\n")
            }
        }
    }
    invisible(path)
}

rules <- function(model)
{
  if (!inherits(model, "rpart")) stop("Not a legitimate rpart tree")

  frm     <- model$frame
  names   <- row.names(frm)
  ylevels <- attr(model, "ylevels")
  ds.size <- model$frame[1,]$n

  rulelist <- character()
  coverage <- numeric()
  #yval <- character()


  for (i in 1:nrow(frm))
  {
    if (frm[i,1] == "<leaf>")
    {
      pth <- mypath(model, nodes=as.numeric(names[i]), print.it=FALSE)
      if(ylevels[frm[i,]$yval] == 1){
        rule <- unname(unlist(pth))[-1]
        rule <- paste(rule, collapse = "_AND_")
       rulelist[length(rulelist)+1] <- rule
       coverage[length(coverage)+1] <- unname(frm[i,]$n)
      }
    }
  }
  #rulelist
  data.frame(rulelist,coverage, stringsAsFactors=FALSE)
}
'''


def merge_rules(rule1, rule2):
    """ Merge the constraints for a single parameter to get a single constraint if possible.

    :param rule1: The original condition.
    :param rule2: The condition that needs to be merged to the original.

    :return: A Single constraint generated by merging the above two.
    """

    # Handle the merging when both operations are equal. And when one is '='
    if rule1['operation'] == '=':
        if rule2['operation'] == '=':
            return {'operation': '=', 'operand': set(rule1['operand']) & set(rule2['operand'])}
        else:
            return {'operation': '=', 'operand': rule1['operand']}
    elif rule2['operation'] == '=':
        return {'operation': '=', 'operand': rule2['operand']}
    elif rule1['operation'] == '<' and rule2['operation'] == '<':
        return {'operation': '<', 'operand': [min([num(rule1['operand']), num(rule2['operand'])])]}
    elif rule1['operation'] == '>' and rule2['operation'] == '>':
        return {'operation': '>', 'operand': [max([num(rule1['operand']), num(rule2['operand'])])]}
    elif rule1['operation'] == '<=' and rule2['operation'] == '<=':
        return {'operation': '<=', 'operand': [min([num(rule1['operand']), num(rule2['operand'])])]}
    elif rule1['operation'] == '>=' and rule2['operation'] == '>=':
        return {'operation': '>=', 'operand': [max([num(rule1['operand']), num(rule2['operand'])])]}

    operations = [rule1['operation'], rule2['operation']]
    operands = [rule1['operand'], rule2['operand']]

    if '>' in operations:
        op_index = operations.index('>')
        # > and <
        if operations[(op_index + 1) % 2] == '<':
            return {'operation': '<>', 'operand': sorted(operands)}
        # > and <=
        if operations[(op_index + 1) % 2] == '<=':
            return {'operation': '<=>', 'operand': sorted(operands)}
        # > and >=
        if operations[(op_index + 1) % 2] == '>=':
            if max(operands) == operands[op_index]:
                return {'operation': '>', 'operand': [max(operands)]}
            else:
                return {'operation': '>=', 'operand': [max(operands)]}
    elif '<' in operations:
        op_index = operations.index('<')
        # < and >=
        if operations[(op_index + 1) % 2] == '>=':
            return {'operation': '<>=', 'operand': sorted(operands)}
        # < and <=
        if operations[(op_index + 1) % 2] == '<=':
            if max(operands) == operands[op_index]:
                return {'operation': '<', 'operand': [min(operands)]}
            else:
                return {'operation': '<=', 'operand': [min(operands)]}
    elif '<=' in operations and '>=' in operations:
        return {'operation': '<=>=', 'operand': [min(operands)]}


def parse_rule(rule_string, aggregate_functions):
    """ Parse a single path through the tree and generate rule objects.

    :param rule_string: The string of the path identified by R.
    :param aggregate_functions: the set of aggregate functions on each parameter of the data set.

    :return: A ConstraintSet object of the single path through the decision tree.
    """
    print rule_string
    rules = rule_string.split('_AND_')

    rule_dict = {}

    for rule in rules:
        if '<=' in rule:
            rule_variable, rule_parameters = rule.split('<=')
            if rule_variable in rule_dict.keys():
                rule_dict[rule_variable] = merge_rules(rule_dict[rule_variable],
                                                       num(rule_parameters.split(',')[-1], rule_variable))
            else:
                rule_dict[rule_variable] = {'operation': '<=',
                                            'operand': num(rule_parameters.split(',')[-1], rule_variable)}
        elif '>=' in rule:
            rule_variable, rule_parameters = rule.split('>=')
            if rule_variable in rule_dict.keys():
                rule_dict[rule_variable] = merge_rules(rule_dict[rule_variable],
                                                       {'operation': '>=',
                                                        'operand': num(rule_parameters.split(',')[-1], rule_variable)})
            else:
                rule_dict[rule_variable] = {'operation': '>=',
                                            'operand': num(rule_parameters.split(',')[-1], rule_variable)}
        elif '=' in rule:
            splits = rule.split('=')
            rule_variable = splits[0]
            rule_parameters = splits[1]
            if rule_variable in rule_dict.keys():
                rule_dict[rule_variable] = merge_rules(rule_dict[rule_variable],
                                                       {'operation': '=', 'operand': rule_parameters.split(',')})
            else:
                rule_dict[rule_variable] = {'operation': '=', 'operand': rule_parameters.split(',')}
        elif '<' in rule:
            rule_variable, rule_parameters = rule.split('<')
            if rule_variable in rule_dict.keys():
                rule_dict[rule_variable] = merge_rules(rule_dict[rule_variable],
                                                       {'operation': '<',
                                                        'operand': num(rule_parameters.split(',')[-1], rule_variable)})
            else:
                rule_dict[rule_variable] = {'operation': '<',
                                            'operand': num(rule_parameters.split(',')[-1], rule_variable)}
        elif '>' in rule:
            rule_variable, rule_parameters = rule.split('>')
            if rule_variable in rule_dict.keys():
                rule_dict[rule_variable] = merge_rules(rule_dict[rule_variable],
                                                       {'operation': '>',
                                                        'operand': num(rule_parameters.split(',')[-1], rule_variable)})
            else:
                rule_dict[rule_variable] = {'operation': '>',
                                            'operand': num(rule_parameters.split(',')[-1], rule_variable)}

    rule_set = []
    for rule_key in rule_dict:
        rule = rule_dict[rule_key]
        rule_set.append(rule_classes.rules[rule['operation']](rule_key, rule['operand'],
                                                              get_aggregate_function(aggregate_functions, rule_key)))

    return rule_classes.ConstraintSet('AND', rule_set)


def generate(selected_ids, dataframe, state):
    """ Handles generating rules.
    When given a data set and a set of subset of data points generates a CEP query to identify events of the same type.

    IMPORTANT : the respective method should return a valid Pandas data frame
                containing the data of the parsed file.

    :param selected_ids: ids of the events that the user selected as important
    :param dataframe: current data frame
    :param state: A state object that catalogues the operations performed on the data set.

    :return: 'success': True if the rule was successfully generated,
             The generated filter conditions,
             number of selected data items,
             number of items covered by the generated rule,
             precision of the rule,
             recall value of the rule,
             preamble of the rule,
             window of the rule if there is a window applied,
             A list of ids for the false-positives
             A list of ids for the false-negatives
    """
    try:
        success = True
        rules = []
        global data_types
        data_types = dataframe.dtypes
        columns = dataframe.columns
        selected_ids = json.loads(selected_ids)

        temporary_dataframe = dataframe.copy(deep=True)
        temporary_dataframe[constants.RULEGEN_COLUMN_NAME] = 0
        temporary_dataframe[constants.RULEGEN_COLUMN_NAME][
            temporary_dataframe.index.isin(selected_ids['selected_ids'])] = 1

        if 'clusterID' in temporary_dataframe.columns:
            temporary_dataframe = temporary_dataframe.drop('clusterID', 1)

        selected_columns = ' + '.join(selected_ids['checked_columns'])

        r_dataframe = com.convert_to_r_dataframe(temporary_dataframe)
        ro.r(r_code)
        rpart = importr('rpart')

        decision_tree = rpart.rpart(constants.RULEGEN_COLUMN_NAME + ' ~ ' + selected_columns, method="class", data=r_dataframe)


        ro.r.assign('temp', decision_tree)
        r_get_rules_function = ro.globalenv['rules']
        rule_set = com.convert_robj(r_get_rules_function(decision_tree))

        if len(rule_set) == 0:
            return False, None, None, None, None, None, None, None, None

        temp_rule_set = []

        # if debug:
        print "---------------------------------------------------------------------------------"
        print "Decision Tree"
        logger.debug(decision_tree)
        print decision_tree
        print rule_set
        print "---------------------------------------------------------------------------------"

        for index, row in rule_set.iterrows():
            temp_rule = parse_rule(row['rulelist'], state[constants.AGGREGATE_FUNCTION_ON_ATTR])
            temp_rule_set.append(temp_rule)
            print temp_rule
            rules.append({'rule': temp_rule.to_string(), 'coverage': row['coverage']})
            print rules

        # print rules

        constraint_set = rule_classes.ConstraintSet('OR', temp_rule_set)
        rule_applied_index = constraint_set.apply_constraint(temporary_dataframe).index

        temporary_dataframe['FILTERED_BY_RULE'] = 0
        temporary_dataframe['FILTERED_BY_RULE'][
            temporary_dataframe.index.isin(rule_applied_index)] = 1

        # print list(temporary_dataframe[(temporary_dataframe['FILTERED_BY_RULE'] == 0) & (temporary_dataframe[constants.RULEGEN_COLUMN_NAME] == 1)].index)

        confusion_mat = confusion_matrix(temporary_dataframe['FILTERED_BY_RULE'], temporary_dataframe[constants.RULEGEN_COLUMN_NAME])

        precision = get_precision(confusion_mat)
        recall = get_recall(confusion_mat)

        select_string = get_aggregate_string(state[constants.AGGREGATE_FUNCTION_ON_ATTR], columns)
        window_string = get_window_string(state)

        logger.debug(constraint_set.to_string())

        # if debug:
        print "Rule Created - "
        print constraint_set.to_string()
        print "---------------------------------------------------------------------------------"
        print "Size of the Dataset = " + str(len(dataframe))
        print "Number of events selected for rule generation = " + str(len(temporary_dataframe))
        print "Number of events filtered through the rule = " + str(len(constraint_set.apply_constraint(temporary_dataframe).index))
        print "---------------------------------------------------------------------------------"
        print "Confusion Matrix - "
        print confusion_mat
        print "Precision = " + str(precision) + ", recall = " + str(recall)

        number_selected = len(selected_ids['selected_ids'])
        number_identified = len(rule_applied_index)
        false_positives = list(temporary_dataframe[(temporary_dataframe['FILTERED_BY_RULE'] == 1) & (temporary_dataframe[constants.RULEGEN_COLUMN_NAME] == 0)].index)
        return success, rules, number_selected, number_identified, precision, recall, select_string, window_string, false_positives

    except Exception, e:
        print e
        return False, None, None, None, None, None, None, None, None


def get_precision(conf_matrix):
    """ Returns the precision of the rule when applied to the data set.

    :param conf_matrix: confusion matrix generated by matching the user selected ids against the ids
                        filtered through the rule.
    :return: The precision calculated by dividing the amount of true positives from the amount of all items
             filtered out by the rule.
    """
    return (conf_matrix[1][1])/float(conf_matrix[1][0] + conf_matrix[1][1])


def get_recall(conf_matrix):
    """ Returns the recall of the rule when applied to the data set.

    :param conf_matrix: confusion matrix generated by matching the user selected ids against the ids
                        filtered through the rule.
    :return: The recall calculated by dividing the amount of all positives from the amount of all
             items selected by the user.
    """
    return (conf_matrix[1][1])/float(conf_matrix[0][1]+conf_matrix[1][1])


def num(input_string, column):
    """ Converts Strings to appropriate number formats .

    :param input_string: The string that need to be converted
    :param column: The name of the parameter that is being considered.
    :return: The converted value in datetime format if the column is a date column. else the float or int value
    """
    try:
        if data_types[column] == 'datetime64[ns]':
            return datetime.datetime.fromtimestamp(float(input_string))
        return int(input_string)
    except ValueError:
        return float(input_string)


def get_aggregate_function(state, column):
    """ Returns the aggregate function applied to a column if any.

    :param state: current state of the data. should be a valid state object.
    :return: The aggregate function applied to generate the column.
    """
    try:
        return state[column][0]
    except Exception, e:
        return ''


def get_aggregate_string(state, columns):
    """ Returns a CEP query element that can perform aggregate functions.

    :param state: current state of the data. should be a valid state object.
    :param columns: the columns that need to be selected from the rule
    :return: A String of the CEP query that can be used to perform these aggregate functions.
    """
    temp_col = []
    for i in xrange(len(columns)):
        aggregate = get_aggregate_function(state, columns[i])
        if aggregate != '':
            temp_col.append(aggregate + '(' + columns[i] + ') AS ' + aggregate + '_' + columns[i])
        else:
            temp_col.append(columns[i])

    return ', '.join(temp_col)


def get_window_string(state_map):
    """ Returns a CEP query element that can be used to perform a windowing operation.

    :param state_map: Current state of the data. should be a valid state object.
    :return: A String of the CEP query that can be used to perform the windowing function.
    """
    if not (state_map[constants.WINDOW_TYPE] is None):
        window_string = '#window.' + state_map[constants.WINDOW_TYPE]
        if state_map[constants.WINDOW_TYPE] == constants.TIME_WINDOW:
            window_string = window_string + '(' + str(state_map[constants.TIME_WINDOW_VALUE]) + ' ' + str(
                state_map[constants.TIME_GRANULARITY]) + ')'
        else:
            window_string = window_string + '(' + str(state_map[constants.EVENT_WINDOW_VALUE]) + ')'

        return window_string
    else:
        return ''